{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwnK2ghV9FCU"
   },
   "source": [
    "# Prompt Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoConfig\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "os.environ[\"MPLBACKEND\"] = \"Agg\"\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg', force=True)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare dataset\n",
    "data = []\n",
    "with open(\"sec-desc.jsonl\", 'r') as file:\n",
    "    for i in file:\n",
    "        data.append(json.loads(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tuining the embedding model through a classification head\n",
    "class ModuleEmbedderHead(nn.Module):\n",
    "    def __init__(self, embedding_model):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(embedding_model)\n",
    "\n",
    "     #Classification head that output logits\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1))\n",
    "\n",
    "    #Encoding function\n",
    "    def encoding(self, inputs):\n",
    "        outputs = self.encoder(**inputs)\n",
    "        #CLS Embeddings\n",
    "        embedding = outputs.last_hidden_state[:, 0]\n",
    "        return embedding\n",
    "\n",
    "    #Classification function\n",
    "    def classifying(self, inputs):\n",
    "        return self.classifier(inputs)\n",
    "\n",
    "    #Forward function\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.encoding(inputs)\n",
    "        output_logits = self.classifying(embeddings)\n",
    "        return output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Classifier function\n",
    "def train_classifier(model, tokenizer, dataloader, data, epochs=5, lr=2e-5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    #Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    #Initialize loss\n",
    "    bce= nn.BCEWithLogitsLoss()\n",
    "\n",
    "    torch.manual_seed(42);\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(dataloader):\n",
    "            safe=[]\n",
    "            unsafe=[]\n",
    "            for i in batch:\n",
    "                safe.append(i[\"func_src_after\"])\n",
    "                unsafe.append(i[\"func_src_before\"])\n",
    "            #Tokenize the inputs\n",
    "            safe_tokens = tokenizer(safe, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "            unsafe_tokens = tokenizer(unsafe, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "            #Foward pass to get the logits\n",
    "            pos_logits = model(safe_tokens)\n",
    "            neg_logits = model(unsafe_tokens)\n",
    "\n",
    "            #Get true labels (1 if safe and 0 if unsafe)\n",
    "            pos_labels = torch.ones_like(pos_logits)\n",
    "            labels_neg = torch.zeros_like(neg_logits)\n",
    "\n",
    "            #Calculate total loss\n",
    "            safe_loss = bce(pos_logits, pos_labels)\n",
    "            unsafe_loss = bce(neg_logits, labels_neg)\n",
    "            loss = safe_loss + unsafe_loss\n",
    "\n",
    "            #Update model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Avg Loss = {total_loss / len(data)}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"classifier.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train and test\n",
    "train_triplets, test_triplets = train_test_split(\n",
    "    data, test_size=0.2, random_state=42)\n",
    "batch_size = 1\n",
    "\n",
    "#Prepare data for train\n",
    "dataloader = DataLoader(train_triplets, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "\n",
    "#Define the embedding model\n",
    "embedding_model = \"microsoft/graphcodebert-base\"\n",
    "#Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
    "#Initialize classifier\n",
    "classifier_model = ModuleEmbedderHead(embedding_model)\n",
    "#Train\n",
    "train_classifier(classifier_model, tokenizer , dataloader, train_triplets, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a module that learns a small number of soft prompt to be prepended to the input\n",
    "class SoftPrompt(nn.Module):\n",
    "    def __init__(self, num_virtual_tokens, embed_dim):\n",
    "        super().__init__()\n",
    "        #Initizalize a learnable matrix of shape (num_virtual_tokens,embed_dim) - randomly initialized\n",
    "        self.embedding = nn.Embedding(num_virtual_tokens, embed_dim)\n",
    "\n",
    "    def forward(self, batch_size):\n",
    "        #Create a 1D tensor\n",
    "        indices = torch.arange(self.embedding.num_embeddings)\n",
    "        #Add a batch dimension and replicate and put tensor on the same device of the weights\n",
    "        indices = indices.unsqueeze(0).repeat(batch_size, 1).to(self.embedding.weight.device)\n",
    "        #return a tensor of shape (batch_size, num_virtual_tokens, embed_dim) that can be prepended to the input\n",
    "        embeddings=self.embedding(indices)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create semantic memory\n",
    "class Memory:\n",
    "    def __init__(self, max_size=5000):\n",
    "        self.memory = deque(maxlen=max_size)\n",
    "\n",
    "    #Add to memory method\n",
    "    def add(self, embedding, reward, description, code):\n",
    "        self.memory.append({\"embedding\": embedding.detach().cpu(), \"reward\": reward, \"description\": description, \"code\": code})\n",
    "\n",
    "    #Retrieve the best n past similar embeddings method\n",
    "    def get_past_embeddings(self, query, top_n, min_reward=0.5):\n",
    "\n",
    "        #If the memory is empty return an empty list\n",
    "        if len(self.memory) == 0:\n",
    "            return []\n",
    "\n",
    "        # Filter memory by reward\n",
    "        filter_memory = []\n",
    "        for i in self.memory:\n",
    "            if i[\"reward\"] >= min_reward:\n",
    "                filter_memory.append(i)\n",
    "\n",
    "        #If there is no safe memory return empty list\n",
    "        if len(filter_memory) == 0:\n",
    "            return []\n",
    "\n",
    "        #Get only the list of embeddings\n",
    "        past_embeddings = []\n",
    "        for i in filter_memory:\n",
    "            past_embeddings.append(i[\"embedding\"])\n",
    "\n",
    "        #Convert to tensor\n",
    "        past_embeddings = torch.stack(past_embeddings)\n",
    "\n",
    "        #Compute cosine similarity between query and embeddings\n",
    "        cos_similarity = F.cosine_similarity(query.cpu().unsqueeze(0), past_embeddings, dim=1)\n",
    "\n",
    "        #Get top-n most similar indices\n",
    "        topn_indices = torch.topk(cos_similarity, k=min(top_n, len(cos_similarity)))[1]\n",
    "\n",
    "        #Save the code in text form of the best most semantically similar embeddings\n",
    "        best_memory = []\n",
    "        for i in topn_indices:\n",
    "            best_memory.append(filter_memory[i][\"code\"])\n",
    "        #Return best\n",
    "        return best_memory\n",
    "\n",
    "\n",
    "    #Build augmented prompt function\n",
    "    def augment_prompt(self, description, desc_embed, top_n=3):\n",
    "\n",
    "        #Get best past examples\n",
    "        best_past_examples = self.get_past_embeddings(desc_embed, top_n=top_n)\n",
    "\n",
    "        #Build augmented prompt with the past safe code in text form \n",
    "        context = []\n",
    "        for i in best_past_examples:\n",
    "            context.append(f\"# Past sampple:\\n{i}\\n\")\n",
    "        augmented_prompt = \"\\n\".join(context)\n",
    "\n",
    "        #Contenate total prompt\n",
    "        full_prompt = augmented_prompt + f\"\\n# Task:\\n{description}\"\n",
    "\n",
    "        return full_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_soft_prompt(model, soft_prompt, dataloader, tokenizer, classifier, classifier_tokenizer, semantic_memory, num_epochs=3, lr=1e-3, alpha=0.9):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    soft_prompt.to(device)\n",
    "\n",
    "    #Load model and freeze it\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    model.eval().to(device)\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    #Load classifier for inference and optimizer\n",
    "    classifier.eval().to(device)\n",
    "    optimizer = torch.optim.Adam(soft_prompt.parameters(), lr=lr)\n",
    "\n",
    "    #Initialize metrics logs\n",
    "    loss_log=[]\n",
    "    sim_safe_log=[]\n",
    "    sim_unsafe_log=[]\n",
    "    reward_log =[]\n",
    "\n",
    "    #Initialize baseline\n",
    "    baseline = 0.0\n",
    "\n",
    "    torch.manual_seed(42);\n",
    "    for epoch in range(num_epochs):\n",
    "        soft_prompt.train()\n",
    "        epoch_loss = 0\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch in tqdm(dataloader):\n",
    "           desc=[]\n",
    "           safe=[]\n",
    "           unsafe=[]\n",
    "           for i in batch:\n",
    "                desc= [i[\"description\"]]\n",
    "                safe= [i[\"func_src_after\"]]\n",
    "                unsafe= [i[\"func_src_before\"]]\n",
    "\n",
    "           #Tokenize description and target\n",
    "           inputs = tokenizer(desc, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "           target = tokenizer(safe, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "           #Get input embeddings\n",
    "           input_embeds=classifier.encoding(inputs).squeeze(0)\n",
    "\n",
    "           #Generate a batch of virtual soft embeddings of the same size of the batch (1)\n",
    "           soft_prompts = soft_prompt(1)\n",
    "\n",
    "           #Build augmented Prompt\n",
    "           augmented_prompt = semantic_memory.augment_prompt(description=desc, desc_embed=input_embeds, top_n=3)\n",
    "\n",
    "           #Tokenize full prompt\n",
    "           inputs_concat = tokenizer(augmented_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "           #Get full input embeddings\n",
    "           input_embeds2 = model.get_input_embeddings()(inputs_concat.input_ids)\n",
    "\n",
    "           #Concatenate learnable emebeddings to full input embeddings\n",
    "           full_embeds = torch.cat([soft_prompts, input_embeds2], dim=1)\n",
    "\n",
    "           #Construct attention mask of 1s\n",
    "           prompt_mask = torch.ones(1, soft_prompts.size(1), device=device)\n",
    "           full_attention_mask = torch.cat([prompt_mask, inputs_concat.attention_mask], dim=1)\n",
    "\n",
    "\n",
    "           #Forward pass\n",
    "           outputs = model(inputs_embeds=full_embeds, attention_mask=full_attention_mask, labels=target.to(device))\n",
    "           loss = outputs.loss\n",
    "\n",
    "           #Sample the generated output\n",
    "           gen_ids= model.generate(\n",
    "                   inputs_embeds=full_embeds, attention_mask=full_attention_mask,\n",
    "                   do_sample=True, temperature=0.7, top_k=50)\n",
    "\n",
    "           #Decode the generated output\n",
    "           text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "           #Tokenize the generated code using the classifier tokenizer\n",
    "           gen_inputs = classifier_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "           #Classify and get reward (probability of being safe)\n",
    "           with torch.no_grad():\n",
    "               logit = classifier(gen_inputs).squeeze()\n",
    "               reward = torch.sigmoid(logit).item()\n",
    "\n",
    "           #REINFORCE update\n",
    "           reward_adj = reward - baseline\n",
    "           loss_reinforce = reward_adj * loss\n",
    "\n",
    "           #EMA Baseline update\n",
    "           baseline = alpha * baseline + (1 - alpha) * reward\n",
    "\n",
    "           #Similarity metrics\n",
    "           sim_safe = F.cosine_similarity(\n",
    "            classifier.encoding(gen_inputs).squeeze(0),\n",
    "            classifier.encoding(classifier_tokenizer(safe, return_tensors=\"pt\", truncation=True, padding=True).to(device)).squeeze(0),\n",
    "            dim=0).item()\n",
    "           sim_unsafe = F.cosine_similarity(\n",
    "                classifier.encoding(gen_inputs).squeeze(0),\n",
    "                classifier.encoding(classifier_tokenizer(unsafe, return_tensors=\"pt\", truncation=True, padding=True).to(device)).squeeze(0),\n",
    "                dim=0).item()\n",
    "\n",
    "\n",
    "           #Tokenize the generated text\n",
    "           gen_inputs = classifier_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "           #Encode the generated text\n",
    "           embeds = classifier.encoding(gen_inputs).squeeze(0)\n",
    "           #Add to memory\n",
    "           semantic_memory.add(embedding=embeds, reward=reward, description=desc, code=text)\n",
    "\n",
    "\n",
    "           loss = loss_reinforce\n",
    "           loss.backward()\n",
    "           #Update prompt tuner parameters\n",
    "           optimizer.step()\n",
    "           #Clear gradients to no accumulate\n",
    "           optimizer.zero_grad()\n",
    "\n",
    "\n",
    "           #Update metrics\n",
    "           epoch_loss += loss_reinforce.item()\n",
    "           loss_log.append(loss_reinforce.item())\n",
    "           sim_safe_log.append(sim_safe)\n",
    "           sim_unsafe_log.append(sim_unsafe)\n",
    "           reward_log.append(reward)\n",
    "\n",
    "           print(f\"Loss: {loss.item()} | Reward: {reward} | Safe Sim: {sim_safe} | Unsafe Sim: {sim_unsafe}\")\n",
    "\n",
    "        print(f\"Avg {epoch+1} Epoch Loss: {epoch_loss / len(dataloader):}\")\n",
    "\n",
    "    return loss_log, sim_safe_log, sim_unsafe_log, reward_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the prompt Tuner\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "num_soft_tokens = 10\n",
    "batch_size = 1\n",
    "dataloader = DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "\n",
    "#Initialize prompt tuner\n",
    "soft_prompt= SoftPrompt(num_soft_tokens, config.d_model)\n",
    "\n",
    "#Initialize classifier\n",
    "classifier = ModuleEmbedderHead(\"microsoft/graphcodebert-base\")\n",
    "classifier.load_state_dict(torch.load(\"classifier.pt\"))\n",
    "classifier.eval()\n",
    "classifier_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "\n",
    "\n",
    "semantic_memory = Memory()\n",
    "\n",
    "loss_log, sim_safe_log, sim_unsafe_log, reward_log= train_soft_prompt(\n",
    "    model_name, soft_prompt, dataloader, tokenizer, classifier, classifier_tokenizer, semantic_memory=semantic_memory, num_epochs=20, alpha=0.9)\n",
    "\n",
    "torch.save(soft_prompt.state_dict(), \"prompt_tuner.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot training metrics\n",
    "steps = list(range(1, len(loss_log) + 1))\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(steps, loss_log, linestyle=\"-\", marker=\"o\", markersize=4, color=\"steelblue\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"REINFORCE Loss\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(steps, reward_log, linestyle=\"-\", marker=\"o\", markersize=4, color=\"#FFDAB9\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Probability of safe code\")\n",
    "plt.title(\"Reward Over Time\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(steps, sim_safe_log, linestyle=\"-\", marker=\"o\", markersize=4, color=\"slategray\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Safe Similarity\")\n",
    "plt.title(\"Similarity to Safe Code\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(steps, sim_unsafe_log, linestyle=\"-\", marker=\"o\", markersize=4, color=\"#D7BDE2\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Unsafe Similarity\")\n",
    "plt.title(\"Similarity to Unsafe Code\")\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_stats.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot average training metrics per epoch \n",
    "num_epochs=20\n",
    "\n",
    "avg_loss=[]\n",
    "avg_reward= []\n",
    "avg_sim_safe=[]\n",
    "avg_sim_unsafe= []\n",
    "\n",
    "for epoch_losses, epoch_rewards, epoch_safe, epoch_unsafe in zip(\n",
    "    np.array_split(loss_log, num_epochs),\n",
    "    np.array_split(reward_log, num_epochs),\n",
    "    np.array_split(sim_safe_log, num_epochs),\n",
    "    np.array_split(sim_unsafe_log, num_epochs)):\n",
    "\n",
    "    avg_loss.append(np.mean(epoch_losses))\n",
    "    avg_reward.append(np.mean(epoch_rewards))\n",
    "    avg_sim_safe.append(np.mean(epoch_safe))\n",
    "    avg_sim_unsafe.append(np.mean(epoch_unsafe))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), avg_loss, marker='o', color=\"steelblue\")\n",
    "plt.title(\"Avg Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Avg Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), avg_reward, marker='o', color=\"#FFDAB9\")\n",
    "plt.title(\"Avg Classifier Reward per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Avg Reward\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(range(1, num_epochs + 1), avg_sim_safe, marker='o', color=\"slategray\")\n",
    "plt.title(\"Avg Safe Similarity per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cosine Similarity to Safe Code\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(range(1, num_epochs + 1), avg_sim_unsafe, marker='o', color=\"#D7BDE2\")\n",
    "plt.title(\"Avg Unsafe Similarity per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cosine Similarity to Unsafe Code\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"epochwise_metrics.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Take embeddings and rewards\n",
    "emb=[]\n",
    "for i in semantic_memory.memory:\n",
    "    emb.append(torch.stack([i[\"embedding\"]]))\n",
    "rew=[]\n",
    "for i in semantic_memory.memory:\n",
    "    emb.append(np.array([i[\"reward\"]]))\n",
    "\n",
    "#Normalize rewards\n",
    "rew_norm = (rew - rew.min()) / (rew.max() - rew.min() + 1e-8)\n",
    "#t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "result = tsne.fit_transform(emb)\n",
    "#Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(result[:, 0], result[:, 1], c=rew_norm, cmap='viridis', s=20)\n",
    "plt.colorbar(scatter, label=\"Normalized Reward\")\n",
    "plt.title(\"Visualization of Semantic Memory Embeddings\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid()\n",
    "plt.savefig(\"TSNE.png\",  dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
